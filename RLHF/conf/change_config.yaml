
model: 
  model_size: "70m" # 70m 160m 410m 1b 1.4b 2.8b 6.9b 12b 
  model_name: "EleutherAI/pythia-70m" # PPO for sft e.g., "EleutherAI/pythia-410m" "lomahony/pythia-${model.model_size}-helpful-sft"

tokenizer: 
  tokenizer_name: "${model.model_name}"

train: 
  batch_size: 32
  total_steps: 1000000
  checkpoint_interval: 1000
  eval_interval: 500
  checkpoint_dir: "/n/holyscratch01/hlakkaraju_lab/Lab/aaronli/RLHF_models/sft_hh/${model.model_name}"
  project_name: "pythia-sft"
  group_name: "${model.model_size}"
  run_name: "${model.model_name}_${optimizer.name}_${optimizer.lr}"
  seed: 0
  save_optimizer: false 

method:
  max_new_tokens: 128
  gen_kwargs:
    top_k: 20

optimizer: 
  name: "adamw" # "rmsprop" # "adamw" adamw_8bit_bnb
  # for adamw
  betas: (0.9, 0.95) # 
  eps: 1.0e-8 #
  weight_decay: 1.0e-6 #
  lr: 1.0e-6

scheduler: 
  name: "cosine_annealing"
  kwargs:
    eta_min: 1.0e-6
